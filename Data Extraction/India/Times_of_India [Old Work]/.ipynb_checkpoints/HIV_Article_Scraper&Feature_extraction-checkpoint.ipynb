{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from newsplease import NewsPlease\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tree import Tree\n",
    "from string import digits\n",
    "from nltk import tokenize\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import nltk, string\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://timesofindia.indiatimes.com/topic/hiv/'\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "# Souping function\n",
    "def Soup(url):\n",
    "    response = get(url)\n",
    "    return BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Scraping total no.of pages contians in TimesOfIndia website about HIV articles\n",
    "def pagecount(url):\n",
    "    soup_obj = Soup(url)\n",
    "    return int(soup_obj.select('a.look')[-1].text)\n",
    "\n",
    "# Extract links from each page to scrap about HIV articles\n",
    "def Get_links(url):\n",
    "    links = []\n",
    "    for count in range(pagecount(url)):\n",
    "        soup_obj = Soup(url + str(count))\n",
    "        block = ['https://timesofindia.indiatimes.com/topic' + item.a['href'] for item in soup_obj.select('.content')]\n",
    "        links.extend(block)\n",
    "    links = list(set(links))\n",
    "    return links\n",
    "\n",
    "# Prepares a data frame with the required attributes\n",
    "def Df_builder(links):\n",
    "    titles, publish_dates, authors, urls, text = [], [], [], [], []\n",
    "    for index, item in enumerate(links):\n",
    "        # Appending Article Content\n",
    "        try:\n",
    "            sys.stdout.write(\"\\r\" + str(index) + \" : \" +\"Articles has Extracted\" + \"\\r\")\n",
    "            sys.stdout.flush()\n",
    "            article = NewsPlease.from_url(item)\n",
    "            text.append(article.text)\n",
    "        except:\n",
    "            continue\n",
    "    data_frame = pd.DataFrame({\"Text\" : text})\n",
    "    return data_frame\n",
    "\n",
    "# Extracting short phrases from text\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "\n",
    "# Filtering text from removing the punctuation, digits, stopwords from it\n",
    "def get_filtered_text(text):\n",
    "    lowers = text.lower()\n",
    "    ent = get_continuous_chunks(text)\n",
    "    k = []\n",
    "    for e in ent:\n",
    "        for ea in e.lower().split():\n",
    "            k.append(ea)\n",
    "    resultwords  = [i for i in lowers.split() if i not in k]\n",
    "    res = ' '.join(resultwords)\n",
    "    res = res.translate(remove_digits)\n",
    "    table = str.maketrans({key: None for key in string.punctuation}) \n",
    "    no_punctuation = res.translate(table)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "links = Get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 : Articles has Extracted\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MG\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname IST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 : Articles has Extracted\r"
     ]
    }
   ],
   "source": [
    "df = Df_builder(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = df.dropna()\n",
    "corpus = list(data['Text'])\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=0.2, use_idf=True, tokenizer=get_filtered_text)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus) #fit the vectorizer to synopses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 58)\n"
     ]
    }
   ],
   "source": [
    "# matrix shape\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aids', 'also', 'among', 'blood', 'cases', 'centre', 'control', 'could', 'day', 'department', 'director', 'disease', 'district', 'dr', 'even', 'family', 'first', 'found', 'get', 'government', 'health', 'hospital', 'however', 'including', 'india', 'infected', 'infection', 'last', 'like', 'living', 'made', 'many', 'medical', 'national', 'new', 'number', 'one', 'patients', 'people', 'positive', 'said', 'since', 'society', 'state', 'take', 'tested', 'three', 'time', 'treatment', 'two', 'virus', 'woman', 'would', 'year', 'years', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words feature extraction\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
