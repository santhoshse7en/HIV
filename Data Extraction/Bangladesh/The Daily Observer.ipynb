{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIV News articles extraction from The Daily Observer. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published Date\n",
    "- Category\n",
    "- Publication\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions\n",
    "from dateutil import parser #handling the date format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for HIV News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls, category, publication = [], [], [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search results¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'HIV site:www.observerbd.com'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "try:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 680 results (0.23 seconds)\n",
    "    max_pages = round([int(s) for s in soup.select_one('div#resultStats').text.split() if s.isdigit()][0]/10)\n",
    "    max_pages = max_pages + 1\n",
    "except:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 1,080 results (0.23 seconds)\n",
    "    max_pages = round(int(''.join(i for i in soup.select_one('div#resultStats').text if i.isdigit()))/10)\n",
    "    max_pages = max_pages + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 : 75\r"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Extracted URL's are : 231 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(\"Total Extracted URL's are\" + ' : ' + str(len(urls)), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 20sserverbd.com/details.php?id=99263ppg=20pg=3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease\n",
    "        soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            headlines.append(article.title.strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            descriptions.append(' '.join(article.meta_data['og']['description'].split()))\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            authors.append(article.authors.strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            dt = parser.parse(soup.select_one('.pub').span.text.split(\":\")[1].split(\"at\")[0])\n",
    "            dates.append(str(dt).split()[0])\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news category\n",
    "        try:\n",
    "            category.append(article.meta_data['category'])\n",
    "        except:\n",
    "            category.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            news.append(' '.join(article.text.split()).replace(\"\\'\\'\",\" \").replace(\"\\'\", \"\").replace(\" / \", \"\"))\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts the news publication\n",
    "        try:\n",
    "            publication.append(article.meta_data['og']['site_name'])\n",
    "        except:\n",
    "            publication.append(None)\n",
    "\n",
    "        # Extracts Keywords and Summaries\n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(' '.join(article.summary.split()))\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "                        \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        category.append(None)\n",
    "        publication.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 231 231 231 231 231 231 231 231 231\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(category), len(publication), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and droping the missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Articles</th>\n",
       "      <th>category</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>Source_URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London HIV patient becomes world's second AIDS...</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>The Daily Observer</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[london, patient, hope, man, hiv, cure, gupta,...</td>\n",
       "      <td>We can't detect anything,\" said Ravindra Gupta...</td>\n",
       "      <td>https://www.observerbd.com/details.php?id=186899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90 infected by HIV syringe</td>\n",
       "      <td>KARACHI, Apr 3: At least 90 people, including ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>The Daily Observer</td>\n",
       "      <td>KARACHI, Apr 3: At least 90 people, including ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[children, 90, hiv, using, 65, syringe, doctor...</td>\n",
       "      <td>KARACHI, Apr 3: At least 90 people, including ...</td>\n",
       "      <td>https://www.observerbd.com/details.php?id=196017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Third HIV patient ‘cured’ of virus after bone ...</td>\n",
       "      <td>LONDON, Mar 9: A third, previously HIV-positiv...</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>The Daily Observer</td>\n",
       "      <td>Third HIV patient ‘cured’ of virus after bone ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[london, patient, cells, case, ccr5, hiv, syst...</td>\n",
       "      <td>Third HIV patient ‘cured’ of virus after bone ...</td>\n",
       "      <td>http://www.observerbd.com/details.php?id=187553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daily Observer</td>\n",
       "      <td>Most Popular Online Newportal in Bangladesh</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Daily Observer</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[london, observer, cleared, man, daily, mar, k...</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "      <td>http://www.observerbd.com/cat.php?cd=1&amp;key=HIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HIV patient gifts kidney</td>\n",
       "      <td>WASHINGTON, Mar 29: The kidney of a 35-year-ol...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>The Daily Observer</td>\n",
       "      <td>WASHINGTON, Mar 29: The kidney of a 35-year-ol...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[virus, patient, woman, washington, kidney, gi...</td>\n",
       "      <td>WASHINGTON, Mar 29: The kidney of a 35-year-ol...</td>\n",
       "      <td>http://www.observerbd.com/details.php?id=190823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0  London HIV patient becomes world's second AIDS...   \n",
       "1                         90 infected by HIV syringe   \n",
       "2  Third HIV patient ‘cured’ of virus after bone ...   \n",
       "3                                     Daily Observer   \n",
       "4                           HIV patient gifts kidney   \n",
       "\n",
       "                                        Descriptions Authors Published_Dates  \\\n",
       "0  London, Mar 5: An HIV-positive man in Britain ...    None      2019-03-06   \n",
       "1  KARACHI, Apr 3: At least 90 people, including ...    None            None   \n",
       "2  LONDON, Mar 9: A third, previously HIV-positiv...    None      2019-03-10   \n",
       "3        Most Popular Online Newportal in Bangladesh    None            None   \n",
       "4  WASHINGTON, Mar 29: The kidney of a 35-year-ol...    None            None   \n",
       "\n",
       "          Publication                                           Articles  \\\n",
       "0  The Daily Observer  London, Mar 5: An HIV-positive man in Britain ...   \n",
       "1  The Daily Observer  KARACHI, Apr 3: At least 90 people, including ...   \n",
       "2  The Daily Observer  Third HIV patient ‘cured’ of virus after bone ...   \n",
       "3      Daily Observer  London, Mar 5: An HIV-positive man in Britain ...   \n",
       "4  The Daily Observer  WASHINGTON, Mar 29: The kidney of a 35-year-ol...   \n",
       "\n",
       "  category                                           Keywords  \\\n",
       "0       {}  [london, patient, hope, man, hiv, cure, gupta,...   \n",
       "1       {}  [children, 90, hiv, using, 65, syringe, doctor...   \n",
       "2       {}  [london, patient, cells, case, ccr5, hiv, syst...   \n",
       "3       {}  [london, observer, cleared, man, daily, mar, k...   \n",
       "4       {}  [virus, patient, woman, washington, kidney, gi...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  We can't detect anything,\" said Ravindra Gupta...   \n",
       "1  KARACHI, Apr 3: At least 90 people, including ...   \n",
       "2  Third HIV patient ‘cured’ of virus after bone ...   \n",
       "3  London, Mar 5: An HIV-positive man in Britain ...   \n",
       "4  WASHINGTON, Mar 29: The kidney of a 35-year-ol...   \n",
       "\n",
       "                                        Source_URLs  \n",
       "0  https://www.observerbd.com/details.php?id=186899  \n",
       "1  https://www.observerbd.com/details.php?id=196017  \n",
       "2   http://www.observerbd.com/details.php?id=187553  \n",
       "3    http://www.observerbd.com/cat.php?cd=1&key=HIV  \n",
       "4   http://www.observerbd.com/details.php?id=190823  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(headlines) == len(descriptions) == len(authors) == len(dates) == len(news) == len(publication) == len(keywords) == len(summaries) == len(urls) == len(category):\n",
    "    tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                        'Descriptions' : descriptions,\n",
    "                        'Authors' : authors,\n",
    "                        'Published_Dates' : dates,\n",
    "                        'Publication' : publication,\n",
    "                        'Articles' : news,\n",
    "                        'category' : category,\n",
    "                        'Keywords' : keywords,\n",
    "                        'Summaries' : summaries,\n",
    "                        'Source_URLs' : urls})\n",
    "    tbl.dropna()\n",
    "    path = 'D:\\\\#Backups\\\\Desktop\\\\!Code!\\\\CDRI\\\\HIV\\\\Data Extraction\\\\#Datasets\\\\'\n",
    "    tbl.to_csv(path+'The_Daily_Observer.csv', index=False)\n",
    "else:\n",
    "    print('Array lenght does not match!')\n",
    "\n",
    "tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
