{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIV News articles extraction from The Daily Observer. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published_Date\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for HIV News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls = [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search resultsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'HIV site:www.observerbd.com'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "try:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 680 results (0.23 seconds)\n",
    "    max_pages = round([int(s) for s in soup.select_one('div#resultStats').text.split() if s.isdigit()][0]/10)\n",
    "    max_pages = max_pages + 1\n",
    "except:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 1,080 results (0.23 seconds)\n",
    "    max_pages = round(int(''.join(i for i in soup.select_one('div#resultStats').text if i.isdigit()))/10)\n",
    "    max_pages = max_pages + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 : 51\r"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(len(urls), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 11sserverbd.com/2015/11/28/123131.phpb_comment_id=1325795027501456_1326639780750314\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease \n",
    "        soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            try:\n",
    "                headlines.append(article.title.strip())\n",
    "            except:\n",
    "                headlines.append(soup.select_one('meta[property=\"twitter:title\"]')['content'].strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            try:\n",
    "                descriptions.append(article.meta_description.strip())\n",
    "            except:\n",
    "                descriptions.append(soup.select_one('meta[property=\"og:description\"]')['content'].strip().replace('\\n', ' '))\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            try:\n",
    "                authors.append(article.authors.strip())\n",
    "            except:\n",
    "                authors.append(soup.select_one('meta[name=\"cXenseParse:tss-byline\"]')['content'].strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            try:\n",
    "                dates.append(str(article.publish_date))\n",
    "            except:\n",
    "                dates.append(soup.select_one('meta[name=\"cXenseParse:recs:publishtime\"]')['content'].strip())\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            try:\n",
    "                news.append(article.text.strip())\n",
    "            except:\n",
    "                news.append(soup.select_one('div#content').text.replace('\\n', '').strip())\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts Keywords and Summaries    \n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(article.summary)\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "            \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 218 218 218 218 218 218 218\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and droping the missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London HIV patient becomes world's second AIDS...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "      <td>[worlds, hiv, aids, patient, gupta, man, known...</td>\n",
       "      <td>We can't detect anything,\" said Ravindra Gupta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daily Observer</td>\n",
       "      <td>The Daily Observer is one of the most reliable...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "      <td>[cleared, daily, worldwide, mar, man, adult, k...</td>\n",
       "      <td>London, Mar 5: An HIV-positive man in Britain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HIV used to cure 'bubble boy' disease</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>HIV used to cure 'bubble boy' disease\\n\\nResul...</td>\n",
       "      <td>[used, hiv, immune, bone, st, cure, disease, b...</td>\n",
       "      <td>HIV used to cure 'bubble boy' diseaseResults o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some Rohingyas with HIV identified</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Many Rohingya refugee children and adults have...</td>\n",
       "      <td>[diseases, hiv, hospital, treatment, refugee, ...</td>\n",
       "      <td>When asked about the status of the Rohingya ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HIV's ability to cause AIDS is weakening over ...</td>\n",
       "      <td>Rapid evolution of HIV, the human immunodefici...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-12-02 00:00:00</td>\n",
       "      <td>HIV's ability to cause AIDS is weakening over ...</td>\n",
       "      <td>[virulent, aids, hiv, cause, weakening, scient...</td>\n",
       "      <td>HIV's ability to cause AIDS is weakening over ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0  London HIV patient becomes world's second AIDS...   \n",
       "1                                     Daily Observer   \n",
       "2              HIV used to cure 'bubble boy' disease   \n",
       "3                 Some Rohingyas with HIV identified   \n",
       "4  HIV's ability to cause AIDS is weakening over ...   \n",
       "\n",
       "                                        Descriptions Authors  \\\n",
       "0                                                       None   \n",
       "1  The Daily Observer is one of the most reliable...    None   \n",
       "2                                                       None   \n",
       "3                                                       None   \n",
       "4  Rapid evolution of HIV, the human immunodefici...    None   \n",
       "\n",
       "       Published_Dates                                           Articles  \\\n",
       "0                 None  London, Mar 5: An HIV-positive man in Britain ...   \n",
       "1                 None  London, Mar 5: An HIV-positive man in Britain ...   \n",
       "2                 None  HIV used to cure 'bubble boy' disease\\n\\nResul...   \n",
       "3                 None  Many Rohingya refugee children and adults have...   \n",
       "4  2014-12-02 00:00:00  HIV's ability to cause AIDS is weakening over ...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [worlds, hiv, aids, patient, gupta, man, known...   \n",
       "1  [cleared, daily, worldwide, mar, man, adult, k...   \n",
       "2  [used, hiv, immune, bone, st, cure, disease, b...   \n",
       "3  [diseases, hiv, hospital, treatment, refugee, ...   \n",
       "4  [virulent, aids, hiv, cause, weakening, scient...   \n",
       "\n",
       "                                           Summaries  \n",
       "0  We can't detect anything,\" said Ravindra Gupta...  \n",
       "1  London, Mar 5: An HIV-positive man in Britain ...  \n",
       "2  HIV used to cure 'bubble boy' diseaseResults o...  \n",
       "3  When asked about the status of the Rohingya ch...  \n",
       "4  HIV's ability to cause AIDS is weakening over ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                    'Descriptions' : descriptions,\n",
    "                    'Authors' : authors,\n",
    "                    'Published_Dates' : dates, \n",
    "                    'Articles' : news,\n",
    "                    'Keywords' : keywords,\n",
    "                    'Summaries' : summaries,})\n",
    "#tbl = tbl.dropna()\n",
    "tbl.to_csv('The_Daily_Observer.csv', index=False)\n",
    "tbl.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
