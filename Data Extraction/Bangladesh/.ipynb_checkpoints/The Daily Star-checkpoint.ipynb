{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIV News articles extraction from The Daily Star. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published Date\n",
    "- Category\n",
    "- Publication\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for HIV News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls, category, publication = [], [], [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search resultsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'HIV site:www.thedailystar.net'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "try:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 680 results (0.23 seconds)\n",
    "    max_pages = round([int(s) for s in soup.select_one('div#resultStats').text.split() if s.isdigit()][0]/10)\n",
    "    max_pages = max_pages + 1\n",
    "except:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 1,080 results (0.23 seconds)\n",
    "    max_pages = round(int(''.join(i for i in soup.select_one('div#resultStats').text if i.isdigit()))/10)\n",
    "    max_pages = max_pages + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 : 186\r"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Extracted URL's are : 291 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(\"Total Extracted URL's are\" + ' : ' + str(len(urls)), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.thedailystar.net/law/2008/02/02/index.htm on URL https://www.thedailystar.net/law/2008/02/02/index.htm\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.thedailystar.net/2005/04/20/d504201502130.htm on URL http://www.thedailystar.net/2005/04/20/d504201502130.htm\n",
      "You must `download()` an article first!th/frozen-shoulder-symptoms-you-should-know-1612555delhi-today0371ad-use-1655662\n",
      "Wall time: 25min 14sedailystar.net/news/with-assistance-we-can-take-dramatic-turnaround0525\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            headlines.append(article.title.strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            descriptions.append(' '.join(article.meta_data['og']['description'].split()))\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            authors.append(article.authors.strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            dates.append(article.meta_data['article']['published_time'].split('T')[0])\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news category\n",
    "        try:\n",
    "            category.append(article.meta_data['category'])\n",
    "        except:\n",
    "            category.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            news.append(' '.join(article.text.split()).replace(\"\\'\\'\",\" \").replace(\"\\'\", \"\").replace(\" / \", \"\"))\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts the news publication\n",
    "        try:\n",
    "            publication.append(article.meta_data['og']['site_name'])\n",
    "        except:\n",
    "            publication.append(None)\n",
    "\n",
    "        # Extracts Keywords and Summaries\n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(' '.join(article.summary.split()))\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "                        \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        category.append(None)\n",
    "        publication.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291 291 291 291 291 291 291 291 291 291\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(category), len(publication), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and droping the missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Articles</th>\n",
       "      <th>category</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>Source_URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>869 diagnosed with HIV in the country this year</td>\n",
       "      <td>A total of 869 people affected by Human Immuno...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>The Daily Star</td>\n",
       "      <td>A total of 869 people affected by Human Immuno...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[patients, virus, health, dermatology, country...</td>\n",
       "      <td>A total of 869 people affected by Human Immuno...</td>\n",
       "      <td>https://www.thedailystar.net/backpage/news/869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like a phoenix: living with HIV for 32 years</td>\n",
       "      <td>It was the hottest day of summer Europe ever e...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-18</td>\n",
       "      <td>The Daily Star</td>\n",
       "      <td>It was the hottest day of summer Europe ever e...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[phoenix, usa, 32, haerry, hiv, treatment, liv...</td>\n",
       "      <td>We were there to talk about decriminalisation ...</td>\n",
       "      <td>https://www.thedailystar.net/health/living-wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>London HIV patient becomes world's second AIDS...</td>\n",
       "      <td>An HIV-positive man in Britain has become the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>The Daily Star</td>\n",
       "      <td>An HIV-positive man in Britain has become the ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[worlds, virus, known, patient, man, hope, don...</td>\n",
       "      <td>We can't detect anything,\" said Ravindra Gupta...</td>\n",
       "      <td>https://www.thedailystar.net/health/news/londo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HIV deaths: Bangladesh 10th in Asia</td>\n",
       "      <td>Bangladesh stands at the tenth position in HIV...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>The Daily Star</td>\n",
       "      <td>Bangladesh stands at the tenth position in HIV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[2016, deaths, seen, asia, hiv, disease, 10th,...</td>\n",
       "      <td>Bangladesh stands at the tenth position in HIV...</td>\n",
       "      <td>https://www.thedailystar.net/health/disease-hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Living with HIV: a fight to the death</td>\n",
       "      <td>Little Akib was in the last stages of advanced...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>The Daily Star</td>\n",
       "      <td>Little Akib was in the last stages of advanced...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[status, plha, death, hiv, hivpositive, living...</td>\n",
       "      <td>For those with HIV, the immune system is suppr...</td>\n",
       "      <td>https://www.thedailystar.net/star-weekend/livi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0    869 diagnosed with HIV in the country this year   \n",
       "1       Like a phoenix: living with HIV for 32 years   \n",
       "2  London HIV patient becomes world's second AIDS...   \n",
       "3                HIV deaths: Bangladesh 10th in Asia   \n",
       "4              Living with HIV: a fight to the death   \n",
       "\n",
       "                                        Descriptions Authors Published_Dates  \\\n",
       "0  A total of 869 people affected by Human Immuno...    None      2018-12-02   \n",
       "1  It was the hottest day of summer Europe ever e...    None      2018-11-18   \n",
       "2  An HIV-positive man in Britain has become the ...    None      2019-03-05   \n",
       "3  Bangladesh stands at the tenth position in HIV...    None      2018-01-26   \n",
       "4  Little Akib was in the last stages of advanced...    None      2017-12-01   \n",
       "\n",
       "      Publication                                           Articles category  \\\n",
       "0  The Daily Star  A total of 869 people affected by Human Immuno...       {}   \n",
       "1  The Daily Star  It was the hottest day of summer Europe ever e...       {}   \n",
       "2  The Daily Star  An HIV-positive man in Britain has become the ...       {}   \n",
       "3  The Daily Star  Bangladesh stands at the tenth position in HIV...       {}   \n",
       "4  The Daily Star  Little Akib was in the last stages of advanced...       {}   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [patients, virus, health, dermatology, country...   \n",
       "1  [phoenix, usa, 32, haerry, hiv, treatment, liv...   \n",
       "2  [worlds, virus, known, patient, man, hope, don...   \n",
       "3  [2016, deaths, seen, asia, hiv, disease, 10th,...   \n",
       "4  [status, plha, death, hiv, hivpositive, living...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  A total of 869 people affected by Human Immuno...   \n",
       "1  We were there to talk about decriminalisation ...   \n",
       "2  We can't detect anything,\" said Ravindra Gupta...   \n",
       "3  Bangladesh stands at the tenth position in HIV...   \n",
       "4  For those with HIV, the immune system is suppr...   \n",
       "\n",
       "                                         Source_URLs  \n",
       "0  https://www.thedailystar.net/backpage/news/869...  \n",
       "1  https://www.thedailystar.net/health/living-wit...  \n",
       "2  https://www.thedailystar.net/health/news/londo...  \n",
       "3  https://www.thedailystar.net/health/disease-hi...  \n",
       "4  https://www.thedailystar.net/star-weekend/livi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(headlines) == len(descriptions) == len(authors) == len(dates) == len(news) == len(publication) == len(keywords) == len(summaries) == len(urls) == len(category):\n",
    "    tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                        'Descriptions' : descriptions,\n",
    "                        'Authors' : authors,\n",
    "                        'Published_Dates' : dates,\n",
    "                        'Publication' : publication,\n",
    "                        'Articles' : news,\n",
    "                        'category' : category,\n",
    "                        'Keywords' : keywords,\n",
    "                        'Summaries' : summaries,\n",
    "                        'Source_URLs' : urls})\n",
    "    tbl.dropna()\n",
    "    path = 'D:\\\\#Backups\\\\Desktop\\\\!Code!\\\\CDRI\\\\HIV\\\\Data Extraction\\\\#Datasets\\\\'\n",
    "    tbl.to_csv(path+'The_Daily_Star.csv', index=False)\n",
    "else:\n",
    "    print('Array lenght does not match!')\n",
    "\n",
    "tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
