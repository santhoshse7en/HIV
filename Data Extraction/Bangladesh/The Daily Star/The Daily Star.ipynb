{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIV News articles extraction from The Daily Star. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published_Date\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for HIV News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls = [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search resultsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'HIV site:www.thedailystar.net'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "try:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 680 results (0.23 seconds)\n",
    "    max_pages = round([int(s) for s in soup.select_one('div#resultStats').text.split() if s.isdigit()][0]/10)\n",
    "    max_pages = max_pages + 1\n",
    "except:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 1,080 results (0.23 seconds)\n",
    "    max_pages = round(int(''.join(i for i in soup.select_one('div#resultStats').text if i.isdigit()))/10)\n",
    "    max_pages = max_pages + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 : 189\r"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(len(urls), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31min 44sedailystar.net/news/media-can-dispel-ostracism91prevention-1221439sh-130918-todaygnostic-tests-1581874un-1436092\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease \n",
    "        soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            try:\n",
    "                headlines.append(soup.select_one('meta[property=\"og:title\"]')['content'].strip())\n",
    "            except:\n",
    "                headlines.append(article.title.strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            try:\n",
    "                descriptions.append(soup.select_one('meta[property=\"og:description\"]')['content'].strip().replace('\\n', ' '))\n",
    "            except:\n",
    "                descriptions.append(article.meta_description.strip())\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            try:\n",
    "                authors.append(soup.select_one('.detailed-content').select_one('span[itemprop=\"name\"]').a.text.strip())\n",
    "            except:\n",
    "                authors.append(article.authors.strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            try:\n",
    "                dates.append(soup.select_one('meta[property=\"article:published_time\"]')['content'].strip())\n",
    "            except:\n",
    "                dates.append(str(article.publish_date))\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            try:\n",
    "                news.append(soup.select_one('.node-content').text.replace('\\n', '').strip())\n",
    "            except:\n",
    "                news.append(article.text.strip())\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts Keywords and Summaries    \n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(article.summary)\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "            \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 286 286 286 286 286 286 286\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and droping the missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>869 diagnosed with HIV in the country this year</td>\n",
       "      <td>A total of 869 people affected by Human Immuno...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-02T00:00:00+06:00</td>\n",
       "      <td>A total of 869 people affected by Human Immuno...</td>\n",
       "      <td>[869, bangladesh, patients, aids, virus, healt...</td>\n",
       "      <td>A total of 869 people affected by Human Immuno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HIV infects one teenage girl every 3 minutes: UN</td>\n",
       "      <td>Every three minutes, a girl between the ages o...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-07-26T11:16:02+06:00</td>\n",
       "      <td>Every three minutes, a girl between the ages o...</td>\n",
       "      <td>[girl, 15, girls, sex, epidemic, 19, infected,...</td>\n",
       "      <td>Girls and young women made up two-thirds of 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like a phoenix: living with HIV for 32 years</td>\n",
       "      <td>It was the hottest day of summer Europe ever e...</td>\n",
       "      <td>Dr Maruf Hasan</td>\n",
       "      <td>2018-11-18T00:00:00+06:00</td>\n",
       "      <td>It was the hottest day of summer Europe ever e...</td>\n",
       "      <td>[usa, phoenix, worked, treated, treatment, hae...</td>\n",
       "      <td>We were there to talk about decriminalisation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London HIV patient becomes world's second AIDS...</td>\n",
       "      <td>An HIV-positive man in Britain has become the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-03-05T16:41:04+06:00</td>\n",
       "      <td>An HIV-positive man in Britain has become the ...</td>\n",
       "      <td>[second, donor, case, patient, london, aids, m...</td>\n",
       "      <td>We can't detect anything,\" said Ravindra Gupta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HIV deaths: Bangladesh 10th in Asia</td>\n",
       "      <td>Bangladesh stands at the tenth position in HIV...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-26T11:23:58+06:00</td>\n",
       "      <td>Photo: DataLEADS/ Asia News NetworkBangladesh ...</td>\n",
       "      <td>[2016, disease, related, bangladesh, infection...</td>\n",
       "      <td>Bangladesh stands at the tenth position in HIV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0    869 diagnosed with HIV in the country this year   \n",
       "1   HIV infects one teenage girl every 3 minutes: UN   \n",
       "2       Like a phoenix: living with HIV for 32 years   \n",
       "3  London HIV patient becomes world's second AIDS...   \n",
       "4                HIV deaths: Bangladesh 10th in Asia   \n",
       "\n",
       "                                        Descriptions         Authors  \\\n",
       "0  A total of 869 people affected by Human Immuno...            None   \n",
       "1  Every three minutes, a girl between the ages o...            None   \n",
       "2  It was the hottest day of summer Europe ever e...  Dr Maruf Hasan   \n",
       "3  An HIV-positive man in Britain has become the ...            None   \n",
       "4  Bangladesh stands at the tenth position in HIV...            None   \n",
       "\n",
       "             Published_Dates  \\\n",
       "0  2018-12-02T00:00:00+06:00   \n",
       "1  2018-07-26T11:16:02+06:00   \n",
       "2  2018-11-18T00:00:00+06:00   \n",
       "3  2019-03-05T16:41:04+06:00   \n",
       "4  2018-01-26T11:23:58+06:00   \n",
       "\n",
       "                                            Articles  \\\n",
       "0  A total of 869 people affected by Human Immuno...   \n",
       "1  Every three minutes, a girl between the ages o...   \n",
       "2  It was the hottest day of summer Europe ever e...   \n",
       "3  An HIV-positive man in Britain has become the ...   \n",
       "4  Photo: DataLEADS/ Asia News NetworkBangladesh ...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [869, bangladesh, patients, aids, virus, healt...   \n",
       "1  [girl, 15, girls, sex, epidemic, 19, infected,...   \n",
       "2  [usa, phoenix, worked, treated, treatment, hae...   \n",
       "3  [second, donor, case, patient, london, aids, m...   \n",
       "4  [2016, disease, related, bangladesh, infection...   \n",
       "\n",
       "                                           Summaries  \n",
       "0  A total of 869 people affected by Human Immuno...  \n",
       "1  Girls and young women made up two-thirds of 15...  \n",
       "2  We were there to talk about decriminalisation ...  \n",
       "3  We can't detect anything,\" said Ravindra Gupta...  \n",
       "4  Bangladesh stands at the tenth position in HIV...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                    'Descriptions' : descriptions,\n",
    "                    'Authors' : authors,\n",
    "                    'Published_Dates' : dates, \n",
    "                    'Articles' : news,\n",
    "                    'Keywords' : keywords,\n",
    "                    'Summaries' : summaries,})\n",
    "#tbl = tbl.dropna()\n",
    "tbl.to_csv('The_Daily_Star.csv', index=False)\n",
    "tbl.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
